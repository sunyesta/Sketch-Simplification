{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing2\n",
      "SIZE = 64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/sketch_simplification/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from PIL import Image\n",
    "import json\n",
    "import torch\n",
    "import pathlib\n",
    "from pathlib import Path\n",
    "import PixelEncoder\n",
    "from typing import Tuple, Dict, List\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "import math\n",
    "from math import sqrt\n",
    "from tqdm.auto import tqdm\n",
    "import PixelEncoder\n",
    "from PixelEncoder import PixelEncodingSet, Segments\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GLOBAL VARIABLES\n",
    "\n",
    "data_path = Path(\n",
    "    f\"/Users/mary/Documents/School/Sketch Simplification/Sketch-Simplification/dataset\"\n",
    ")\n",
    "\n",
    "flower_data_path = Path(\n",
    "    f\"/Users/mary/Documents/School/Sketch Simplification/Sketch-Simplification/disposable/data\"\n",
    ")\n",
    "\n",
    "device = \"mps\"\n",
    "\n",
    "T = 300\n",
    "IMG_SIZE = 64\n",
    "BATCH_SIZE = 125\n",
    "\n",
    "\n",
    "def linear_beta_schedule(timesteps, start=0.0001, end=0.02):\n",
    "    return torch.linspace(start, end, timesteps)\n",
    "\n",
    "\n",
    "# Define beta schedule\n",
    "T = 300\n",
    "betas = linear_beta_schedule(timesteps=T)\n",
    "\n",
    "# Pre-calculate different terms for closed form\n",
    "alphas = 1.0 - betas\n",
    "alphas_cumprod = torch.cumprod(alphas, axis=0)\n",
    "alphas_cumprod_prev = F.pad(alphas_cumprod[:-1], (1, 0), value=1.0)\n",
    "sqrt_recip_alphas = torch.sqrt(1.0 / alphas)\n",
    "sqrt_alphas_cumprod = torch.sqrt(alphas_cumprod)\n",
    "sqrt_one_minus_alphas_cumprod = torch.sqrt(1.0 - alphas_cumprod)\n",
    "posterior_variance = betas * (1.0 - alphas_cumprod_prev) / (1.0 - alphas_cumprod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<PixelEncoder.PixelEncodingSet at 0x17aedbf10>,\n",
       " <PixelEncoder.Segments at 0x17f6c35b0>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class SketchData(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    Custom PyTorch dataset for image and JSON pairs.\n",
    "\n",
    "    Args:\n",
    "        root_dir (str): Path to the root directory containing image folders.\n",
    "        transform (callable, optional): A function/transform to apply to the image.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, targ_dir, transform=None):\n",
    "        self.paths = list(targ_dir.glob(\"*/.\"))\n",
    "\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        \"Returns one sample of data, data and label (X, y).\"\n",
    "\n",
    "        data_path = self.paths[index]\n",
    "\n",
    "        img = Image.open(data_path / Path(\"base.png\")).convert(\"L\")\n",
    "        assert img.size == (IMG_SIZE, IMG_SIZE)\n",
    "        if self.transform:\n",
    "            # return data, label (X, y)\n",
    "            return self.transform(img), index\n",
    "        else:\n",
    "            return img, index\n",
    "\n",
    "    def getOtherData(self, index: int):\n",
    "        data_path = self.paths[index]\n",
    "        pixe_set = PixelEncodingSet.load(data_path / Path(\"pixe_set.json\"))\n",
    "        segs = Segments.load(data_path / Path(\"segs.json\"), fromBlender=False)\n",
    "        return pixe_set, segs\n",
    "\n",
    "    def getSegs(self,index:int):\n",
    "        data_path = self.paths[index]\n",
    "        segs = Segments.load(data_path / Path(\"segs.json\"), fromBlender=False)\n",
    "        return segs\n",
    "\n",
    "data = SketchData(targ_dir=data_path)\n",
    "data.getOtherData(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # FLOWERS DATASET\n",
    "\n",
    "# data = torchvision.datasets.data = torchvision.datasets.Flowers102(\n",
    "#     root=flower_data_path,\n",
    "#     download=True,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num params:  62437666\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "import math\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, time_emb_dim, up=False):\n",
    "        super().__init__()\n",
    "        self.time_mlp = nn.Linear(time_emb_dim, out_ch)\n",
    "        if up:\n",
    "            self.conv1 = nn.Conv2d(2 * in_ch, out_ch, 3, padding=1)\n",
    "            self.transform = nn.ConvTranspose2d(out_ch, out_ch, 4, 2, 1)\n",
    "        else:\n",
    "            self.conv1 = nn.Conv2d(in_ch, out_ch, 3, padding=1)\n",
    "            self.transform = nn.Conv2d(out_ch, out_ch, 4, 2, 1)\n",
    "        self.conv2 = nn.Conv2d(out_ch, out_ch, 3, padding=1)\n",
    "        self.bnorm1 = nn.BatchNorm2d(out_ch)\n",
    "        self.bnorm2 = nn.BatchNorm2d(out_ch)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x,\n",
    "        t,\n",
    "    ):\n",
    "        # First Conv\n",
    "        h = self.bnorm1(self.relu(self.conv1(x)))\n",
    "        # Time embedding\n",
    "        time_emb = self.relu(self.time_mlp(t))\n",
    "        # Extend last 2 dimensions\n",
    "        time_emb = time_emb[(...,) + (None,) * 2]\n",
    "        # Add time channel\n",
    "        h = h + time_emb\n",
    "        # Second Conv\n",
    "        h = self.bnorm2(self.relu(self.conv2(h)))\n",
    "        # Down or Upsample\n",
    "        return self.transform(h)\n",
    "\n",
    "\n",
    "class SinusoidalPositionEmbeddings(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, time):\n",
    "        device = time.device\n",
    "        half_dim = self.dim // 2\n",
    "        embeddings = math.log(10000) / (half_dim - 1)\n",
    "        embeddings = torch.exp(torch.arange(\n",
    "            half_dim, device=device) * -embeddings)\n",
    "        embeddings = time[:, None] * embeddings[None, :]\n",
    "        embeddings = torch.cat((embeddings.sin(), embeddings.cos()), dim=-1)\n",
    "        # TODO: Double check the ordering here\n",
    "        return embeddings\n",
    "\n",
    "\n",
    "class SimpleUnet(nn.Module):\n",
    "    \"\"\"\n",
    "    A simplified variant of the Unet architecture.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        image_channels = 1\n",
    "        down_channels = (64, 128, 256, 512, 1024)\n",
    "        up_channels = (1024, 512, 256, 128, 64)\n",
    "        out_dim = 2\n",
    "        time_emb_dim = 32\n",
    "\n",
    "        # Time embedding\n",
    "        self.time_mlp = nn.Sequential(\n",
    "            SinusoidalPositionEmbeddings(time_emb_dim),\n",
    "            nn.Linear(time_emb_dim, time_emb_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        # Initial projection\n",
    "        self.conv0 = nn.Conv2d(image_channels, down_channels[0], 3, padding=1)\n",
    "\n",
    "        # Downsample\n",
    "        self.downs = nn.ModuleList(\n",
    "            [\n",
    "                Block(down_channels[i], down_channels[i + 1], time_emb_dim)\n",
    "                for i in range(len(down_channels) - 1)\n",
    "            ]\n",
    "        )\n",
    "        # Upsample\n",
    "        self.ups = nn.ModuleList(\n",
    "            [\n",
    "                Block(up_channels[i], up_channels[i + 1],\n",
    "                      time_emb_dim, up=True)\n",
    "                for i in range(len(up_channels) - 1)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Edit: Corrected a bug found by Jakub C (see YouTube comment)\n",
    "        self.output = nn.Conv2d(up_channels[-1], out_dim, 1)\n",
    "\n",
    "    def forward(self, x, timestep):\n",
    "        # Embedd time\n",
    "        t = self.time_mlp(timestep)\n",
    "        # Initial conv\n",
    "        x = self.conv0(x)\n",
    "        # Unet\n",
    "        residual_inputs = []\n",
    "        for down in self.downs:\n",
    "            x = down(x, t)\n",
    "            residual_inputs.append(x)\n",
    "        for up in self.ups:\n",
    "            residual_x = residual_inputs.pop()\n",
    "            # Add residual x as additional channels\n",
    "            x = torch.cat((x, residual_x), dim=1)\n",
    "            x = up(x, t)\n",
    "        return self.output(x)\n",
    "\n",
    "\n",
    "model = SimpleUnet()\n",
    "print(\"Num params: \", sum(p.numel() for p in model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_to_tensor(data):\n",
    "    data_transforms = transforms.Compose(\n",
    "        [\n",
    "            transforms.Grayscale(num_output_channels=1),\n",
    "            transforms.ToTensor(),\n",
    "        ]\n",
    "    )\n",
    "    return SketchData(data_path, data_transforms)\n",
    "\n",
    "\n",
    "def tensor_to_img(image):\n",
    "    reverse_transforms = transforms.Compose([transforms.ToPILImage()])\n",
    "    return reverse_transforms(image)\n",
    "\n",
    "\n",
    "def show_tensor_image(image):\n",
    "    plt.imshow(tensor_to_img(image))\n",
    "\n",
    "\n",
    "# data = img_to_tensor(data)\n",
    "\n",
    "\n",
    "# plt.figure(figsize=(15, 15))\n",
    "# plt.axis(\"off\")\n",
    "# num_images = 10\n",
    "# stepsize = int(T / num_images)\n",
    "\n",
    "\n",
    "# for idx in range(0, T, stepsize):\n",
    "#     t = torch.Tensor([idx]).type(torch.int64)\n",
    "#     plt.subplot(1, num_images + 1, int(idx / stepsize) + 1)\n",
    "#     img = next(iter(data))[0]\n",
    "#     # print(img)\n",
    "#     show_tensor_image(img)\n",
    "# # print(img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 47\u001b[0m\n\u001b[1;32m     41\u001b[0m dataloader \u001b[38;5;241m=\u001b[39m DataLoader(data, batch_size\u001b[38;5;241m=\u001b[39mBATCH_SIZE,\n\u001b[1;32m     42\u001b[0m                         shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, drop_last\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     44\u001b[0m timesteps \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, T, (BATCH_SIZE,), device\u001b[38;5;241m=\u001b[39mdevice)\u001b[38;5;241m.\u001b[39mlong()\n\u001b[0;32m---> 47\u001b[0m img, img_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(dataloader))[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     49\u001b[0m getRealOffsets(img_index, timesteps)\n\u001b[1;32m     51\u001b[0m out_displacements \u001b[38;5;241m=\u001b[39m model(img\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device), timesteps)\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 1)"
     ]
    }
   ],
   "source": [
    "# ---- Model demo before training ----\n",
    "\n",
    "\n",
    "def applyOffsets(img, displacements):\n",
    "    displacements = displacements.permute(1, 2, 0)\n",
    "    displacements *= IMG_SIZE\n",
    "\n",
    "    def insideImg(x, y):\n",
    "        return x >= 0 and x <= IMG_SIZE and y >= 0 and y <= IMG_SIZE\n",
    "\n",
    "    def mix(v0, v1):\n",
    "        return round(((v0 + v1) / 2))\n",
    "\n",
    "    img_0 = tensor_to_img(img)\n",
    "    img_1 = Image.new(\"L\", (IMG_SIZE, IMG_SIZE), 255)\n",
    "    pixels_0, pixels_1 = img_0.load(), img_1.load()\n",
    "\n",
    "    for x in range(IMG_SIZE):\n",
    "        for y in range(IMG_SIZE):\n",
    "            x_delta, y_delta = displacements[y, x]\n",
    "\n",
    "            x_new, y_new = x + x_delta, y + y_delta\n",
    "            if insideImg(x_new, y_new):\n",
    "                newColor = mix(pixels_0[x, y], pixels_1[x_new, y_new])\n",
    "                # print(newColor)\n",
    "                pixels_1[x_new, y_new] = newColor\n",
    "    return img_1\n",
    "\n",
    "\n",
    "def getRealOffsets(img_index, timesteps):\n",
    "    segs = data.getSegs(img_index)\n",
    "    offsets = PixelEncoder.generateSteppedOffsets(\n",
    "        segs, IMG_SIZE, torch.max(timesteps))\n",
    "\n",
    "    return torch.tensor([offsets[t] for t in timesteps])\n",
    "\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "data = img_to_tensor(data)\n",
    "dataloader = DataLoader(data, batch_size=BATCH_SIZE,\n",
    "                        shuffle=True, drop_last=False)\n",
    "\n",
    "timesteps = torch.randint(0, T, (BATCH_SIZE,), device=device).long()\n",
    "\n",
    "\n",
    "img, img_index = next(iter(dataloader))[0]\n",
    "\n",
    "getRealOffsets(img_index, timesteps)\n",
    "\n",
    "out_displacements = model(img.unsqueeze(0).to(device), timesteps)\n",
    "\n",
    "img_out = applyOffsets(img, out_displacements[0])\n",
    "plt.imshow(img_out)\n",
    "# print(out_img.shape)\n",
    "# show_tensor_image(out_img[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss(model, x_0, t):\n",
    "    x_noisy, noise = forward_diffusion_sample(x_0, t, device)\n",
    "    noise_pred = model(x_noisy, t)\n",
    "    return F.l1_loss(noise, noise_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[54], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch, (img, index) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dataloader):\n\u001b[1;32m     11\u001b[0m     model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m---> 13\u001b[0m     preds \u001b[38;5;241m=\u001b[39m model(\u001b[43mX\u001b[49m)\n\u001b[1;32m     14\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss_fn(preds, y)\n\u001b[1;32m     15\u001b[0m     train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     16\u001b[0m         loss  \u001b[38;5;66;03m# for getting average loss per batch in all batches in this epoch\u001b[39;00m\n\u001b[1;32m     17\u001b[0m     )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "epochs = 3\n",
    "# tqdm is for showing the progress bar\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    print(f\"Epoch: {epoch}\\n------\")\n",
    "\n",
    "    # training\n",
    "\n",
    "    train_loss = 0\n",
    "    # add a loop to loop through all the batches\n",
    "    for batch, (img, index) in enumerate(dataloader):\n",
    "        model.train()\n",
    "\n",
    "        preds = model(X)\n",
    "        loss = loss_fn(preds, y)\n",
    "        train_loss += (\n",
    "            loss  # for getting average loss per batch in all batches in this epoch\n",
    "        )\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 400 == 0:\n",
    "            # len(train_dataloader.dataset) is the total number of samples in our dataset\n",
    "            print(\n",
    "                f\"looked at {batch * len(X)}/{len(train_dataloader.dataset)} samples\")\n",
    "\n",
    "        train_loss /= len(train_dataloader)  # get average loss for the batch\n",
    "\n",
    "    # calculate test data\n",
    "    test_loss, test_acc = 0, 0\n",
    "    model_0.eval()\n",
    "    with torch.inference_mode():\n",
    "        for X_test, y_test in test_dataloader:\n",
    "            test_pred = model_0(X_test)\n",
    "            test_loss += loss_fn(test_pred, y_test)\n",
    "\n",
    "            # test_pred.argmax gives the highest\n",
    "            test_loss += loss_fn(test_pred, y_test)\n",
    "            test_acc += accuracy_fn(y_true=y_test,\n",
    "                                    y_pred=test_pred.argmax(dim=1))\n",
    "\n",
    "        test_loss /= len(test_dataloader)\n",
    "        test_acc /= len(test_dataloader)\n",
    "\n",
    "    # print stats for epoch\n",
    "    print(\n",
    "        f\"Train loss: {train_loss:.4f} | Test loss: {test_loss:.4f}, Test acc: {test_acc}\"\n",
    "    )\n",
    "    end_time = timer()\n",
    "    total_train_time_model_0 = print_train_time(\n",
    "        start=start_time, end=end_time, device=getModelDevice(model_0)\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sketch_simplification",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
