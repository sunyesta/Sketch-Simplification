{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from PIL import Image\n",
    "import json\n",
    "import torch\n",
    "import pathlib\n",
    "from pathlib import Path\n",
    "import PixelEncoder\n",
    "from typing import Tuple, Dict, List\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "import math\n",
    "from math import sqrt\n",
    "from tqdm.auto import tqdm\n",
    "import PixelEncoder\n",
    "from PixelEncoder import PixelEncodingSet, Segments, pointToPixel, pixelToPoint\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "magnitude = np.linalg.norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device =  cuda\n"
     ]
    }
   ],
   "source": [
    "# GLOBAL VARIABLES\n",
    "\n",
    "model_path = r\"C:\\Users\\Mary\\Documents\\Code\\sketch simplification\\Sketch-Simplification\\model/model.pt\"\n",
    "\n",
    "data_path = Path(\n",
    "    r\"C:\\Users\\Mary\\Documents\\Code\\sketch simplification\\Sketch-Simplification\\dataset128\"\n",
    ")\n",
    "\n",
    "flower_data_path = Path(\n",
    "    r\"/Users/mary/Documents/School/Sketch Simplification/Sketch-Simplification/disposable/data\"\n",
    ")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(\"device = \", device)\n",
    "\n",
    "T = 5\n",
    "IMG_SIZE = 128\n",
    "BATCH_SIZE = 1\n",
    "\n",
    "\n",
    "def linear_beta_schedule(timesteps, start=0.0001, end=0.02):\n",
    "    return torch.linspace(start, end, timesteps)\n",
    "\n",
    "\n",
    "# Define beta schedule\n",
    "betas = linear_beta_schedule(timesteps=T)\n",
    "\n",
    "# Pre-calculate different terms for closed form\n",
    "alphas = 1.0 - betas\n",
    "alphas_cumprod = torch.cumprod(alphas, axis=0)\n",
    "alphas_cumprod_prev = F.pad(alphas_cumprod[:-1], (1, 0), value=1.0)\n",
    "sqrt_recip_alphas = torch.sqrt(1.0 / alphas)\n",
    "sqrt_alphas_cumprod = torch.sqrt(alphas_cumprod)\n",
    "sqrt_one_minus_alphas_cumprod = torch.sqrt(1.0 - alphas_cumprod)\n",
    "posterior_variance = betas * \\\n",
    "    (1.0 - alphas_cumprod_prev) / (1.0 - alphas_cumprod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<PixelEncoder.PixelEncodingSet at 0x1f2aa15b160>,\n",
       " <PixelEncoder.Segments at 0x1f3c536dab0>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class SketchData(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    Custom PyTorch dataset for image and JSON pairs.\n",
    "\n",
    "    Args:\n",
    "        root_dir (str): Path to the root directory containing image folders.\n",
    "        transform (callable, optional): A function/transform to apply to the image.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, targ_dir, transform=None):\n",
    "        self.paths = list(targ_dir.glob(\"*/.\"))\n",
    "\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        \"Returns one sample of data, data and label (X, y).\"\n",
    "\n",
    "        data_path = self.paths[index]\n",
    "\n",
    "        img = Image.open(data_path / Path(\"base.png\")).convert(\"L\")\n",
    "\n",
    "        assert img.size == (\n",
    "            IMG_SIZE, IMG_SIZE), f\"Image size must = {IMG_SIZE}\"\n",
    "        if self.transform:\n",
    "            # return data, label (X, y)\n",
    "            return self.transform(img), index\n",
    "        else:\n",
    "            return img, index\n",
    "\n",
    "    def getOtherData(self, index: int):\n",
    "        data_path = self.paths[index]\n",
    "        pixe_set = PixelEncodingSet.load(data_path / Path(\"pixe_set.json\"))\n",
    "        segs = Segments.load(data_path / Path(\"segs.json\"), fromBlender=False)\n",
    "        return pixe_set, segs\n",
    "\n",
    "    def getSegs(self, index: int):\n",
    "        data_path = self.paths[index]\n",
    "        segs = Segments.load(data_path / Path(\"segs.json\"), fromBlender=False)\n",
    "        return segs\n",
    "\n",
    "    def get_pixe_set(self, index: int):\n",
    "        data_path = self.paths[index]\n",
    "        pixe_set = PixelEncodingSet.load(data_path / Path(\"pixe_set.json\"))\n",
    "        return pixe_set\n",
    "\n",
    "\n",
    "data = SketchData(targ_dir=data_path)\n",
    "data.getOtherData(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # FLOWERS DATASET\n",
    "\n",
    "# data = torchvision.datasets.data = torchvision.datasets.Flowers102(\n",
    "#     root=flower_data_path,\n",
    "#     download=True,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 8.00 GiB of which 0 bytes is free. Of the allocated memory 13.53 GiB is allocated by PyTorch, and 46.43 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 137\u001b[0m\n\u001b[0;32m    135\u001b[0m model \u001b[38;5;241m=\u001b[39m SimpleUnet()\n\u001b[0;32m    136\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(model_path):\n\u001b[1;32m--> 137\u001b[0m     state_dict \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    138\u001b[0m     model\u001b[38;5;241m.\u001b[39mload_state_dict(state_dict)\n\u001b[0;32m    140\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNum params: \u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28msum\u001b[39m(p\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mparameters()))\n",
      "File \u001b[1;32mc:\\Users\\Mary\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\serialization.py:1026\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1024\u001b[0m             \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1025\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(UNSAFE_MESSAGE \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(e)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1026\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _load(opened_zipfile,\n\u001b[0;32m   1027\u001b[0m                      map_location,\n\u001b[0;32m   1028\u001b[0m                      pickle_module,\n\u001b[0;32m   1029\u001b[0m                      overall_storage\u001b[38;5;241m=\u001b[39moverall_storage,\n\u001b[0;32m   1030\u001b[0m                      \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n\u001b[0;32m   1031\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mmap:\n\u001b[0;32m   1032\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmmap can only be used with files saved with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1033\u001b[0m                        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`torch.save(_use_new_zipfile_serialization=True), \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1034\u001b[0m                        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mplease torch.save your checkpoint with this option in order to use mmap.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Mary\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\serialization.py:1438\u001b[0m, in \u001b[0;36m_load\u001b[1;34m(zip_file, map_location, pickle_module, pickle_file, overall_storage, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1436\u001b[0m unpickler \u001b[38;5;241m=\u001b[39m UnpicklerWrapper(data_file, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n\u001b[0;32m   1437\u001b[0m unpickler\u001b[38;5;241m.\u001b[39mpersistent_load \u001b[38;5;241m=\u001b[39m persistent_load\n\u001b[1;32m-> 1438\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43munpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1440\u001b[0m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_validate_loaded_sparse_tensors()\n\u001b[0;32m   1441\u001b[0m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_log_api_usage_metadata(\n\u001b[0;32m   1442\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch.load.metadata\u001b[39m\u001b[38;5;124m\"\u001b[39m, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mserialization_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: zip_file\u001b[38;5;241m.\u001b[39mserialization_id()}\n\u001b[0;32m   1443\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Mary\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\serialization.py:1408\u001b[0m, in \u001b[0;36m_load.<locals>.persistent_load\u001b[1;34m(saved_id)\u001b[0m\n\u001b[0;32m   1406\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1407\u001b[0m     nbytes \u001b[38;5;241m=\u001b[39m numel \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_element_size(dtype)\n\u001b[1;32m-> 1408\u001b[0m     typed_storage \u001b[38;5;241m=\u001b[39m \u001b[43mload_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_maybe_decode_ascii\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1410\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m typed_storage\n",
      "File \u001b[1;32mc:\\Users\\Mary\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\serialization.py:1382\u001b[0m, in \u001b[0;36m_load.<locals>.load_tensor\u001b[1;34m(dtype, numel, key, location)\u001b[0m\n\u001b[0;32m   1377\u001b[0m         storage\u001b[38;5;241m.\u001b[39mbyteswap(dtype)\n\u001b[0;32m   1379\u001b[0m \u001b[38;5;66;03m# TODO: Once we decide to break serialization FC, we can\u001b[39;00m\n\u001b[0;32m   1380\u001b[0m \u001b[38;5;66;03m# stop wrapping with TypedStorage\u001b[39;00m\n\u001b[0;32m   1381\u001b[0m typed_storage \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstorage\u001b[38;5;241m.\u001b[39mTypedStorage(\n\u001b[1;32m-> 1382\u001b[0m     wrap_storage\u001b[38;5;241m=\u001b[39m\u001b[43mrestore_location\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m   1383\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[0;32m   1384\u001b[0m     _internal\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   1386\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typed_storage\u001b[38;5;241m.\u001b[39m_data_ptr() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1387\u001b[0m     loaded_storages[key] \u001b[38;5;241m=\u001b[39m typed_storage\n",
      "File \u001b[1;32mc:\\Users\\Mary\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\serialization.py:391\u001b[0m, in \u001b[0;36mdefault_restore_location\u001b[1;34m(storage, location)\u001b[0m\n\u001b[0;32m    389\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_restore_location\u001b[39m(storage, location):\n\u001b[0;32m    390\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _, _, fn \u001b[38;5;129;01min\u001b[39;00m _package_registry:\n\u001b[1;32m--> 391\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    392\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    393\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\Mary\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\serialization.py:271\u001b[0m, in \u001b[0;36m_cuda_deserialize\u001b[1;34m(obj, location)\u001b[0m\n\u001b[0;32m    269\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mUntypedStorage(obj\u001b[38;5;241m.\u001b[39mnbytes(), device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(location))\n\u001b[0;32m    270\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 271\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Mary\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\_utils.py:115\u001b[0m, in \u001b[0;36m_cuda\u001b[1;34m(self, device, non_blocking, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m new_type(indices, values, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msize())\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 115\u001b[0m     untyped_storage \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mUntypedStorage\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    116\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    117\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    118\u001b[0m     untyped_storage\u001b[38;5;241m.\u001b[39mcopy_(\u001b[38;5;28mself\u001b[39m, non_blocking)\n\u001b[0;32m    119\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m untyped_storage\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 8.00 GiB of which 0 bytes is free. Of the allocated memory 13.53 GiB is allocated by PyTorch, and 46.43 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "import math\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, time_emb_dim, up=False):\n",
    "        super().__init__()\n",
    "        self.time_mlp = nn.Linear(time_emb_dim, out_ch)\n",
    "        if up:\n",
    "            self.conv1 = nn.Conv2d(2 * in_ch, out_ch, 3, padding=1)\n",
    "            self.transform = nn.ConvTranspose2d(out_ch, out_ch, 4, 2, 1)\n",
    "        else:\n",
    "            self.conv1 = nn.Conv2d(in_ch, out_ch, 3, padding=1)\n",
    "            self.transform = nn.Conv2d(out_ch, out_ch, 4, 2, 1)\n",
    "        self.conv2 = nn.Conv2d(out_ch, out_ch, 3, padding=1)\n",
    "        self.bnorm1 = nn.BatchNorm2d(out_ch)\n",
    "        self.bnorm2 = nn.BatchNorm2d(out_ch)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x,\n",
    "        t,\n",
    "    ):\n",
    "        # First Conv\n",
    "        h = self.bnorm1(self.relu(self.conv1(x)))\n",
    "        # Time embedding\n",
    "        time_emb = self.relu(self.time_mlp(t))\n",
    "        # Extend last 2 dimensions\n",
    "        time_emb = time_emb[(...,) + (None,) * 2]\n",
    "        # Add time channel\n",
    "        h = h + time_emb\n",
    "        # Second Conv\n",
    "        h = self.bnorm2(self.relu(self.conv2(h)))\n",
    "        # Down or Upsample\n",
    "        return self.transform(h)\n",
    "\n",
    "\n",
    "class SinusoidalPositionEmbeddings(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, time):\n",
    "        device = time.device\n",
    "        half_dim = self.dim // 2\n",
    "        embeddings = math.log(10000) / (half_dim - 1)\n",
    "        embeddings = torch.exp(torch.arange(\n",
    "            half_dim, device=device) * -embeddings)\n",
    "        embeddings = time[:, None] * embeddings[None, :]\n",
    "        embeddings = torch.cat((embeddings.sin(), embeddings.cos()), dim=-1)\n",
    "        # TODO: Double check the ordering here\n",
    "        return embeddings\n",
    "\n",
    "\n",
    "class SimpleUnet(nn.Module):\n",
    "    \"\"\"\n",
    "    A simplified variant of the Unet architecture.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        image_channels = 1\n",
    "        down_channels = (\n",
    "            IMG_SIZE,\n",
    "            IMG_SIZE * 2,\n",
    "            IMG_SIZE * 2**2,\n",
    "            IMG_SIZE * 2**3,\n",
    "            IMG_SIZE * 2**4,\n",
    "        )\n",
    "        up_channels = (\n",
    "            IMG_SIZE * 2**4,\n",
    "            IMG_SIZE * 2**3,\n",
    "            IMG_SIZE * 2**2,\n",
    "            IMG_SIZE * 2,\n",
    "            IMG_SIZE,\n",
    "        )\n",
    "\n",
    "        self.down_channels = down_channels\n",
    "        self.up_channels = up_channels\n",
    "        out_dim = 2\n",
    "        time_emb_dim = 32\n",
    "\n",
    "        # Time embedding\n",
    "        self.time_mlp = nn.Sequential(\n",
    "            SinusoidalPositionEmbeddings(time_emb_dim),\n",
    "            nn.Linear(time_emb_dim, time_emb_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        # Initial projection\n",
    "        self.conv0 = nn.Conv2d(image_channels, down_channels[0], 3, padding=1)\n",
    "\n",
    "        # Downsample\n",
    "        self.downs = nn.ModuleList(\n",
    "            [\n",
    "                Block(down_channels[i], down_channels[i + 1], time_emb_dim)\n",
    "                for i in range(len(down_channels) - 1)\n",
    "            ]\n",
    "        )\n",
    "        # Upsample\n",
    "        self.ups = nn.ModuleList(\n",
    "            [\n",
    "                Block(up_channels[i], up_channels[i + 1],\n",
    "                      time_emb_dim, up=True)\n",
    "                for i in range(len(up_channels) - 1)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Edit: Corrected a bug found by Jakub C (see YouTube comment)\n",
    "        self.output = nn.Conv2d(up_channels[-1], out_dim, 1)\n",
    "\n",
    "    def forward(self, x, timestep):\n",
    "        print(\"down = \", self.down_channels)\n",
    "        print(\"up = \", self.up_channels)\n",
    "\n",
    "        # Embedd time\n",
    "        t = self.time_mlp(timestep)\n",
    "        # Initial conv\n",
    "        x = self.conv0(x)\n",
    "        # Unet\n",
    "        residual_inputs = []\n",
    "        for down in self.downs:\n",
    "            x = down(x, t)\n",
    "            residual_inputs.append(x)\n",
    "        for up in self.ups:\n",
    "            residual_x = residual_inputs.pop()\n",
    "            # Add residual x as additional channels\n",
    "            print(\"shapes = \", x.shape, residual_x.shape)\n",
    "            x = torch.cat((x, residual_x), dim=1)\n",
    "            x = up(x, t)\n",
    "        return self.output(x)\n",
    "\n",
    "\n",
    "model = SimpleUnet()\n",
    "if os.path.exists(model_path):\n",
    "    state_dict = torch.load(model_path)\n",
    "    model.load_state_dict(state_dict)\n",
    "\n",
    "print(\"Num params: \", sum(p.numel() for p in model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data(data):\n",
    "    data_transforms = transforms.Compose(\n",
    "        [\n",
    "            transforms.Grayscale(num_output_channels=1),\n",
    "            transforms.ToTensor(),\n",
    "        ]\n",
    "    )\n",
    "    return SketchData(data_path, data_transforms)\n",
    "\n",
    "\n",
    "def img_to_tensor(img):\n",
    "    data_transforms = transforms.Compose(\n",
    "        [\n",
    "            transforms.Grayscale(num_output_channels=1),\n",
    "            transforms.ToTensor(),\n",
    "        ]\n",
    "    )\n",
    "    return data_transforms(img)\n",
    "\n",
    "\n",
    "def tensor_to_img(image):\n",
    "    reverse_transforms = transforms.Compose([transforms.ToPILImage()])\n",
    "    return reverse_transforms(image)\n",
    "\n",
    "\n",
    "def show_tensor_image(image):\n",
    "    plt.imshow(tensor_to_img(image))\n",
    "\n",
    "\n",
    "# data = img_to_tensor(data)\n",
    "\n",
    "\n",
    "# plt.figure(figsize=(15, 15))\n",
    "# plt.axis(\"off\")\n",
    "# num_images = 10\n",
    "# stepsize = int(T / num_images)\n",
    "\n",
    "\n",
    "# for idx in range(0, T, stepsize):\n",
    "#     t = torch.Tensor([idx]).type(torch.int64)\n",
    "#     plt.subplot(1, num_images + 1, int(idx / stepsize) + 1)\n",
    "#     img = next(iter(data))[0]\n",
    "#     # print(img)\n",
    "#     show_tensor_image(img)\n",
    "# # print(img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Model demo before training ----\n",
    "\n",
    "\n",
    "# TODO fix the offsets\n",
    "def applyOffsets(img, offsets):\n",
    "    # make the offsets in the format (pixel index y, pixel index x, channel)\n",
    "    offsets = offsets.permute(1, 2, 0)\n",
    "    offsets = offsets.cpu().numpy()\n",
    "\n",
    "    # make an array that is filled with points\n",
    "\n",
    "    # apply the offsets to the points, then convert everything to pixels\n",
    "\n",
    "    def insideImg(x, y):\n",
    "        return x >= 0 and x < IMG_SIZE and y >= 0 and y < IMG_SIZE\n",
    "\n",
    "    def mix(v0, v1):\n",
    "        return round(((v0 + v1) / 2))\n",
    "\n",
    "    img_0 = tensor_to_img(img)\n",
    "    img_1 = Image.new(\"L\", (IMG_SIZE, IMG_SIZE), 255)\n",
    "    pixels_0, pixels_1 = img_0.load(), img_1.load()\n",
    "\n",
    "    for x in range(IMG_SIZE):\n",
    "        for y in range(IMG_SIZE):\n",
    "            x_point, y_point = pixelToPoint(\n",
    "                x, IMG_SIZE), pixelToPoint(y, IMG_SIZE)\n",
    "            x_delta, y_delta = offsets[y, x]\n",
    "\n",
    "            x_point_new, y_point_new = x_point + x_delta, y_point + y_delta\n",
    "            x_new, y_new = pointToPixel(x_point_new, IMG_SIZE), pointToPixel(\n",
    "                y_point_new, IMG_SIZE\n",
    "            )\n",
    "            if insideImg(x_new, y_new):\n",
    "                newColor = mix(pixels_0[x, y], pixels_1[x_new, y_new])\n",
    "                pixels_1[x_new, y_new] = newColor\n",
    "    return img_1\n",
    "\n",
    "\n",
    "def getRealOffsets(img_index, timesteps):\n",
    "    # returns the offsets needed to get to the prev timestep\n",
    "    def get_points_at_t(segs, t):\n",
    "        segs_t = PixelEncoder.generateSketchSegs(segs, t / T)\n",
    "        points_t = pixe_set.get_points(segs_t)\n",
    "        return points_t\n",
    "\n",
    "    # init variables\n",
    "    timesteps = timesteps.cpu().numpy()\n",
    "    maxTimestep = timesteps.max()\n",
    "    segs = data.getSegs(img_index)\n",
    "    pixe_set = data.get_pixe_set(img_index)\n",
    "\n",
    "    offsets = []\n",
    "    points_last = get_points_at_t(segs, 0)\n",
    "    offsets.append(np.zeros((IMG_SIZE, IMG_SIZE, 2)))\n",
    "    for t in range(1, maxTimestep + 1):\n",
    "        points_t = get_points_at_t(segs, t)\n",
    "        offsets_t = points_t - points_last\n",
    "        offsets_pixels_t = PixelEncoder.pointToPixel(offsets_t, IMG_SIZE)\n",
    "        offsets_pixels_t = offsets_t.reshape(IMG_SIZE, IMG_SIZE, 2)\n",
    "        offsets.append(offsets_pixels_t)\n",
    "        points_last = points_t\n",
    "\n",
    "    offsets = np.array(offsets)\n",
    "    return torch.tensor([offsets[t] for t in timesteps]).permute(0, 3, 1, 2)\n",
    "\n",
    "\n",
    "def showDifferences(img_index, out_offsets, real_offsets, timesteps, max_dps=5):\n",
    "    segs = data.getSegs(img_index)\n",
    "\n",
    "    dps = min(len(out_offsets), max_dps)\n",
    "    fig, ax = plt.subplots(dps, 2, figsize=(10, 12))\n",
    "\n",
    "    real_offsets_numpy = real_offsets.cpu().numpy()\n",
    "\n",
    "    for i in range(dps):\n",
    "        t = timesteps[i].item()\n",
    "        t_norm = t / T\n",
    "        goal_t_norm = max(0, (t - 1) / T)\n",
    "\n",
    "        segsBaked_start = PixelEncoder.generateSketchSegs(segs, t_norm)\n",
    "        segsBaked_end = PixelEncoder.generateSketchSegs(segs, goal_t_norm)\n",
    "\n",
    "        img_start = img_to_tensor(\n",
    "            PixelEncoder.renderSegsCario(segsBaked_start, IMG_SIZE)\n",
    "        )\n",
    "        img_end = img_to_tensor(\n",
    "            PixelEncoder.renderSegsCario(segsBaked_end, IMG_SIZE))\n",
    "\n",
    "        # ax[i, 0].imshow(tensor_to_img(img_end))\n",
    "        ax[i, 0].imshow(applyOffsets(img_start, out_offsets[i]))\n",
    "        ax[i, 1].imshow(applyOffsets(img_start, real_offsets[i]))\n",
    "        ax[i, 0].set_title(f\"t = { t} generated\")\n",
    "        ax[i, 1].set_title(f\"real\")\n",
    "\n",
    "        # turn off axis\n",
    "        ax[i, 0].get_yaxis().set_visible(False)\n",
    "        ax[i, 0].get_xaxis().set_visible(False)\n",
    "        ax[i, 1].get_yaxis().set_visible(False)\n",
    "        ax[i, 1].get_xaxis().set_visible(False)\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "data = transform_data(data)\n",
    "dataloader = DataLoader(data, batch_size=BATCH_SIZE,\n",
    "                        shuffle=True, drop_last=False)\n",
    "\n",
    "# timesteps = torch.randint(0, T, (BATCH_SIZE,), device=device).long()\n",
    "# timesteps = torch.full((BATCH_SIZE,), 0, device=device).long()\n",
    "timesteps = torch.tensor([1, 2, 3, 4, 5], device=device).long()\n",
    "\n",
    "# get img and img_index\n",
    "img, img_index = next(iter(dataloader))\n",
    "img, img_index = img[0], img_index[0]  # get first item in batch\n",
    "\n",
    "print(\"indexes = \", img)\n",
    "\n",
    "real_offsets = getRealOffsets(img_index, timesteps)\n",
    "\n",
    "\n",
    "out_offsets = None\n",
    "with torch.inference_mode():\n",
    "    out_offsets = model(img.to(device).unsqueeze(0), timesteps)\n",
    "\n",
    "# showDifferences(img, out_offsets, real_offsets, timesteps)\n",
    "showDifferences(img_index, out_offsets, real_offsets, timesteps)\n",
    "\n",
    "# TODO fix to work with multiple datapoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateImagesAtTimesteps(img_index, timesteps):\n",
    "    segs = data.getSegs(img_index)\n",
    "\n",
    "    # convert and normalize timesteps\n",
    "    timesteps = timesteps.cpu().numpy() / T\n",
    "    print(\"timesteps = \", timesteps)\n",
    "\n",
    "    imgs = []\n",
    "    for t in timesteps:\n",
    "        segs_baked = PixelEncoder.generateSketchSegs(segs, t)\n",
    "        img = PixelEncoder.renderSegsCario(segs_baked, IMG_SIZE)\n",
    "        imgs.append(img)\n",
    "    return imgs\n",
    "\n",
    "\n",
    "def get_loss(model, img_index, timesteps):\n",
    "\n",
    "    print(\"img index = \", img_index)\n",
    "    timesteps = timesteps.to(device)\n",
    "    model.to(device)\n",
    "\n",
    "    noisyImgs = [\n",
    "        img_to_tensor(img).cpu().numpy()\n",
    "        for img in generateImagesAtTimesteps(img_index, timesteps)\n",
    "    ]\n",
    "    noisyImgs = torch.tensor(noisyImgs)\n",
    "    offsets_real = getRealOffsets(img_index, timesteps).float()\n",
    "    offsets_pred = model(noisyImgs.to(device), timesteps.to(device))\n",
    "\n",
    "    return F.l1_loss(offsets_real.to(device), offsets_pred.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model():\n",
    "    model_state_dict = model.state_dict()\n",
    "    torch.save(model_state_dict, model_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "\n",
    "model.to(device)\n",
    "optimizer = Adam(model.parameters(), lr=0.001)\n",
    "epochs = 500  # Try more!\n",
    "\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    for step, batch in tqdm(enumerate(dataloader)):\n",
    "        optimizer.zero_grad()\n",
    "        # print(\"batch = \", batch[0]])\n",
    "        imgs, img_indexes = batch\n",
    "\n",
    "        loss = 0\n",
    "        print(\"indexes  = \", img_indexes)\n",
    "        for img_index in img_indexes:\n",
    "            t = torch.randint(0, T, (BATCH_SIZE,), device=device).long()\n",
    "            loss += get_loss(model, img_index, t)\n",
    "\n",
    "        loss /= len(img_indexes)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if step == 0:\n",
    "            print(f\"Epoch {epoch} | step {step:03d} Loss: {loss.item()} \")\n",
    "            save_model()\n",
    "            # sample_plot_image()\n",
    "\n",
    "\n",
    "model_state_dict = model.state_dict()\n",
    "torch.save(model_state_dict, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_state_dict = torch.load(model_path)\n",
    "\n",
    "# model = SimpleUnet()\n",
    "# model.load_state_dict(model_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sketch_simplification",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
